{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Input, concatenate, GlobalAveragePooling2D, AveragePooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "y = df['label'].to_numpy()\n",
    "X = df.drop('label', axis = 1)\n",
    "X = X.to_numpy().reshape(df.shape[0],28,28,1)\n",
    "\n",
    "df_extra = pd.read_csv('Dig-MNIST.csv')\n",
    "y_extra = df_extra['label'].to_numpy()\n",
    "X_extra = df_extra.drop('label', axis = 1)\n",
    "X_extra = X_extra.to_numpy().reshape(df_extra.shape[0],28,28,1)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.10, random_state=777)\n",
    "\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "X_test = df_test.drop('id', axis=1).to_numpy().reshape(df_test.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                                   rotation_range=10,\n",
    "                                   width_shift_range=0.25,\n",
    "                                   height_shift_range=0.25,\n",
    "                                   shear_range=0.1,\n",
    "                                   zoom_range=0.25,\n",
    "                                   horizontal_flip=False)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=5, padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=5, padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(256, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=200,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.2)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=1024),\n",
    "                              steps_per_epoch=100,\n",
    "                              epochs=50,\n",
    "                              validation_data=valid_datagen.flow(X_val, y_val),\n",
    "                              validation_steps=50,\n",
    "                              callbacks=[learning_rate_reduction, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.6617 - accuracy: 0.7746\n",
      "Epoch 00001: val_loss improved from inf to 4.22267, saving model to best_model.h5\n",
      "100/100 [==============================] - 28s 282ms/step - loss: 0.6576 - accuracy: 0.7760 - val_loss: 4.2227 - val_accuracy: 0.0981\n",
      "Epoch 2/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1943 - accuracy: 0.9344\n",
      "Epoch 00002: val_loss did not improve from 4.22267\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 0.1940 - accuracy: 0.9346 - val_loss: 12.5634 - val_accuracy: 0.1013\n",
      "Epoch 3/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9598\n",
      "Epoch 00003: val_loss did not improve from 4.22267\n",
      "100/100 [==============================] - 24s 237ms/step - loss: 0.1232 - accuracy: 0.9598 - val_loss: 9.7599 - val_accuracy: 0.1013\n",
      "Epoch 4/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9659\n",
      "Epoch 00004: val_loss did not improve from 4.22267\n",
      "100/100 [==============================] - 23s 234ms/step - loss: 0.1048 - accuracy: 0.9658 - val_loss: 10.2110 - val_accuracy: 0.1019\n",
      "Epoch 5/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0951 - accuracy: 0.9690\n",
      "Epoch 00005: val_loss improved from 4.22267 to 3.33556, saving model to best_model.h5\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 0.0948 - accuracy: 0.9691 - val_loss: 3.3356 - val_accuracy: 0.3494\n",
      "Epoch 6/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0829 - accuracy: 0.9725\n",
      "Epoch 00006: val_loss improved from 3.33556 to 0.23634, saving model to best_model.h5\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 0.0832 - accuracy: 0.9724 - val_loss: 0.2363 - val_accuracy: 0.9231\n",
      "Epoch 7/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9733\n",
      "Epoch 00007: val_loss improved from 0.23634 to 0.10475, saving model to best_model.h5\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 0.0802 - accuracy: 0.9730 - val_loss: 0.1048 - val_accuracy: 0.9669\n",
      "Epoch 8/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0705 - accuracy: 0.9781\n",
      "Epoch 00008: val_loss improved from 0.10475 to 0.02348, saving model to best_model.h5\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 0.0702 - accuracy: 0.9782 - val_loss: 0.0235 - val_accuracy: 0.9944\n",
      "Epoch 9/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9784\n",
      "Epoch 00009: val_loss did not improve from 0.02348\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0672 - accuracy: 0.9785 - val_loss: 0.0354 - val_accuracy: 0.9912\n",
      "Epoch 10/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9790\n",
      "Epoch 00010: val_loss did not improve from 0.02348\n",
      "100/100 [==============================] - 23s 233ms/step - loss: 0.0641 - accuracy: 0.9789 - val_loss: 0.0453 - val_accuracy: 0.9894\n",
      "Epoch 11/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9809\n",
      "Epoch 00011: val_loss did not improve from 0.02348\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0556 - accuracy: 0.9809 - val_loss: 0.0288 - val_accuracy: 0.9912\n",
      "Epoch 12/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9818\n",
      "Epoch 00012: val_loss did not improve from 0.02348\n",
      "100/100 [==============================] - 23s 235ms/step - loss: 0.0557 - accuracy: 0.9818 - val_loss: 0.0502 - val_accuracy: 0.9869\n",
      "Epoch 13/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0578 - accuracy: 0.9807\n",
      "Epoch 00013: val_loss improved from 0.02348 to 0.02137, saving model to best_model.h5\n",
      "100/100 [==============================] - 25s 246ms/step - loss: 0.0579 - accuracy: 0.9807 - val_loss: 0.0214 - val_accuracy: 0.9956\n",
      "Epoch 14/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0527 - accuracy: 0.9833\n",
      "Epoch 00014: val_loss did not improve from 0.02137\n",
      "100/100 [==============================] - 23s 233ms/step - loss: 0.0536 - accuracy: 0.9831 - val_loss: 0.0430 - val_accuracy: 0.9869\n",
      "Epoch 15/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0523 - accuracy: 0.9832\n",
      "Epoch 00015: val_loss did not improve from 0.02137\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0526 - accuracy: 0.9831 - val_loss: 0.0274 - val_accuracy: 0.9912\n",
      "Epoch 16/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0510 - accuracy: 0.9837\n",
      "Epoch 00016: val_loss did not improve from 0.02137\n",
      "100/100 [==============================] - 23s 231ms/step - loss: 0.0512 - accuracy: 0.9836 - val_loss: 0.0334 - val_accuracy: 0.9906\n",
      "Epoch 17/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9856\n",
      "Epoch 00017: val_loss did not improve from 0.02137\n",
      "100/100 [==============================] - 23s 234ms/step - loss: 0.0459 - accuracy: 0.9856 - val_loss: 0.0261 - val_accuracy: 0.9937\n",
      "Epoch 18/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 0.9845\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02137\n",
      "100/100 [==============================] - 23s 234ms/step - loss: 0.0476 - accuracy: 0.9845 - val_loss: 0.0282 - val_accuracy: 0.9919\n",
      "Epoch 19/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0373 - accuracy: 0.9881\n",
      "Epoch 00019: val_loss improved from 0.02137 to 0.01923, saving model to best_model.h5\n",
      "100/100 [==============================] - 24s 244ms/step - loss: 0.0371 - accuracy: 0.9882 - val_loss: 0.0192 - val_accuracy: 0.9956\n",
      "Epoch 20/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0356 - accuracy: 0.9879\n",
      "Epoch 00020: val_loss improved from 0.01923 to 0.01739, saving model to best_model.h5\n",
      "100/100 [==============================] - 25s 247ms/step - loss: 0.0361 - accuracy: 0.9878 - val_loss: 0.0174 - val_accuracy: 0.9962\n",
      "Epoch 21/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0286 - accuracy: 0.9916\n",
      "Epoch 00021: val_loss did not improve from 0.01739\n",
      "100/100 [==============================] - 24s 236ms/step - loss: 0.0283 - accuracy: 0.9917 - val_loss: 0.0194 - val_accuracy: 0.9950\n",
      "Epoch 22/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9912\n",
      "Epoch 00022: val_loss did not improve from 0.01739\n",
      "100/100 [==============================] - 23s 234ms/step - loss: 0.0272 - accuracy: 0.9913 - val_loss: 0.0184 - val_accuracy: 0.9962\n",
      "Epoch 23/50\n",
      " 99/100 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9914\n",
      "Epoch 00023: val_loss did not improve from 0.01739\n",
      "100/100 [==============================] - 24s 235ms/step - loss: 0.0266 - accuracy: 0.9915 - val_loss: 0.0196 - val_accuracy: 0.9956\n",
      "Epoch 24/50\n",
      " 43/100 [===========>..................] - ETA: 12s - loss: 0.0270 - accuracy: 0.9913"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(128, kernel_size=5, padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(256, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(256, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(512, kernel_size=5, padding='same', activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(512, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                            verbose=1, patience=5)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', verbose=1, patience=10)\n",
    "\n",
    "cp = ModelCheckpoint('best_model.h5', monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "history = model.fit_generator(train_datagen.flow(X_train, y_train, batch_size=256),\n",
    "                              steps_per_epoch=100,\n",
    "                              epochs=50,\n",
    "                              validation_data=valid_datagen.flow(X_val, y_val),\n",
    "                              validation_steps=50,\n",
    "                              callbacks=[learning_rate_reduction, es, cp],\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('best_model.h5')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict_classes(X_test * 1.0 / 255.0)\n",
    "answer = pd.DataFrame(y_pred, columns=['label'])\n",
    "answer.index.name = 'id'\n",
    "answer.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_298 (Conv2D)          (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_299 (Conv2D)          (None, 26, 26, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_46 (MaxPooling (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_300 (Conv2D)          (None, 11, 11, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_301 (Conv2D)          (None, 11, 11, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_47 (MaxPooling (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_302 (Conv2D)          (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_303 (Conv2D)          (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_48 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_12 (Flatten)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,245,770\n",
      "Trainable params: 1,245,002\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test * 1.0 / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer = pd.DataFrame(y_pred, columns=['label'])\n",
    "answer.index.name = 'id'\n",
    "answer.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
